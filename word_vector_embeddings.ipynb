{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to obtain a selection of pre-trained word embeddings that we would like to compare.\n",
    "\n",
    "The candidates are:\n",
    "\n",
    "* Plain `word2vec` from Google trained on the Google News corpus. Vector size 300. Obtained from [the official page](https://code.google.com/archive/p/word2vec/). [Link for the file.](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/)\n",
    "\n",
    "* `GloVe` trained on CommonCrawl 820B and on Wikipedia+Gigaword. Vector size 300. Obtained from [the official page](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "* ELMo Medium version with output size 256. Obtained from [AllenNLP](https://allennlp.org/elmo)\n",
    "\n",
    "* BERT-Base, uncased version, obtained from [Transformers library](https://huggingface.co/transformers/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation (everything except `word2vec`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5\n",
    "!wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json\n",
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.6B.zip\n",
    "!rm glove.6B.zip glove.6B.50d.txt glove.6B.100d.txt glove.6B.200d.txt\n",
    "!unzip glove.840B.300d.zip\n",
    "!rm glove.840B.300d.zip\n",
    "!unzip cased_L-12_H-768_A-12.zip\n",
    "!rm cased_L-12_H-768_A-12.zip\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a samle text file that we will later convert into a point cloud. We will use one text from the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_from_file(filename, keep_punct = False):\n",
    "    with open(filename,\"r\") as file:\n",
    "        text = file.read()\n",
    "    if keep_punct is True:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ' + punct + ' ')\n",
    "    else:\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ')\n",
    "    \n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for x in text.lower().split(' '):\n",
    "        if x.isalpha():\n",
    "            result.append(x)\n",
    "        else:\n",
    "            word = []\n",
    "            for y in x: # for every character\n",
    "                if y.isalpha(): word.append(y)\n",
    "            if len(word) > 0:\n",
    "                result.append(''.join(word))\n",
    "                \n",
    "    return result\n",
    "\n",
    "def get_vectors(wv, words):\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv[w])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Imperialism is a type of advocacy of empire. Its name originated from the Latin word \"imperium\", which means to rule over large territories. Imperialism is \"a policy of extending a country's power and influence through colonization, use of military force, or other means\". Imperialism has greatly shaped the contemporary world. It has also allowed for the rapid spread of technologies and ideas. The term imperialism has been applied to Western (and Japanese) political and economic dominance especially in Asia and Africa in the 19th and 20th centuries. Its precise meaning continues to be debated by scholars. Some writers, such as Edward Said, use the term more broadly to describe any system of domination and subordination organised with an imperial center and a periphery. Imperialism is defined as \"A policy of extending a countryâ€™s power and influence through diplomacy or military force.\" Imperialism is particularly focused on the control that one group, often a state power, has on another group of people. This is often through various forms of \"othering\" (see other) based on racial, religious, or cultural stereotypes. There are \"formal\" or \"informal\" imperialisms. \"Formal imperialism\" is defined as \"physical control or full-fledged colonial rule\". \"Informal imperialism\" is less direct; however, it is still a powerful form of dominance.\"\"\"\n",
    "\n",
    "with open(\"test_text.txt\", \"w\") as file:\n",
    "    file.write(text)\n",
    "    \n",
    "text_words = tokenize_from_file(\"test_text.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors for a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create embeddings for each of the word vector types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `word2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', \\\n",
    "                                                                 binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_output = get_vectors(word2vec_model, text_words)\n",
    "np.save(\"test_word2vec\", word2vec_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GloVe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"glove.6B.300d.gensim.txt\")\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"glove.840B.300d.gensim.txt\")\n",
    "!rm glove.6B.300d.txt glove.840B.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_cc_model = gensim.models.KeyedVectors.load_word2vec_format('glove.840B.300d.gensim.txt', binary=False)\n",
    "glove_wiki_model = gensim.models.KeyedVectors.load_word2vec_format('glove.6B.300d.gensim.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_cc_output = get_vectors(glove_cc_model, text_words)\n",
    "glove_wiki_output = get_vectors(glove_wiki_model, text_words)\n",
    "np.save(\"test_glove_cc\", glove_cc_output)\n",
    "np.save(\"test_glove_wiki\", glove_wiki_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ELMo` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "\n",
    "elmo_embedder = ElmoEmbedder(options_file = \"elmo_2x2048_256_2048cnn_1xhighway_options.json\", \\\n",
    "                        weight_file = \"elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5\")\n",
    "\n",
    "elmo_embeddings_raw = elmo_embedder.embed_sentence(text_words)\n",
    "\n",
    "elmo_embeddings = elmo_embeddings_raw[2]\n",
    "\n",
    "np.save(\"test_elmo\",elmo_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "with open(\"test_text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    \n",
    "    bert_encoded_layers = outputs[0][0]\n",
    "    \n",
    "bert_encoded_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_bert\", bert_encoded_layers.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embedding = np.load(\"test_word2vec.npy\")\n",
    "glove_cc_embedding = np.load(\"test_glove_cc.npy\")\n",
    "glove_wiki_embedding = np.load(\"test_glove_wiki.npy\")\n",
    "elmo_embedding = np.load(\"test_elmo.npy\")\n",
    "bert_embedding = np.load(\"test_bert.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 3.1484153, 5.792499)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "dist_matrix = euclidean_distances(word2vec_embedding)\n",
    "\n",
    "np.min(dist_matrix), np.mean(dist_matrix), np.max(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_perseus(vectors, filename, initial_radii = []):\n",
    "    if len(initial_radii) == 0:\n",
    "        initial_radii = [0.1 for i in range(vectors.shape[1])]\n",
    "    dist_matrix = euclidean_distances(vectors)\n",
    "    vectors = vectors / np.mean(dist_matrix)\n",
    "    with open(filename, \"w\") as out:\n",
    "        out.write(str(vectors.shape[1])+\"\\n\")\n",
    "        out.write(\"1 0.01 100\\n\")\n",
    "        for i, vector in enumerate(vectors):\n",
    "            out.write(\" \".join(map(str,vector))+\" \" +str(initial_radii[i]))\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_to_perseus(word2vec_embedding, filename=\"test_word2vec_perseus\")\n",
    "vectors_to_perseus(glove_cc_embedding, filename=\"test_glove_cc_perseus\")\n",
    "vectors_to_perseus(glove_wiki_embedding,filename=\"test_glove_wiki_perseus\")\n",
    "vectors_to_perseus(elmo_embedding,filename=\"test_elmo_perseus\")\n",
    "vectors_to_perseus(bert_embedding,filename=\"test_bert_perseus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_plex(vectors, filename):\n",
    "#     dist_matrix = euclidean_distances(vectors)\n",
    "#     vectors = vectors / np.mean(dist_matrix)\n",
    "    with open(filename, \"w\") as out:\n",
    "        out.write(\" \".join([str(i) for i in range(vectors.shape[1])]))\n",
    "        out.write(\"\\n\")\n",
    "        for i, vector in enumerate(vectors):\n",
    "            out.write(\" \".join(map(str,vector)))\n",
    "            out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_to_plex(word2vec_embedding, filename=\"test_word2vec_plex\")\n",
    "vectors_to_plex(glove_cc_embedding, filename=\"test_glove_cc_plex\")\n",
    "vectors_to_plex(glove_wiki_embedding, filename=\"test_glove_wiki_plex\")\n",
    "vectors_to_plex(elmo_embedding, filename=\"test_elmo_plex\")\n",
    "vectors_to_plex(bert_embedding, filename=\"test_bert_plex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
